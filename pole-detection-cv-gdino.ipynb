# --- Step 0: Install necessary libraries ---
!pip install -q torch transformers Pillow pandas timm requests

# --- Step 1: Library Imports and Setup ---
import os
import requests
from PIL import Image as PILImage, ImageDraw
from io import BytesIO
import time
from datetime import datetime
import torch
import numpy as np
import math
import pandas as pd
from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
from IPython.display import display
import traceback

# --- Step 2: API Key Configuration ---
API_KEY = None
try:
    from google.colab import userdata
    API_KEY = userdata.get('GOOGLE_API_KEY')
    print(f"[{datetime.now().time()}] ✅ API key configured successfully from Colab Secrets.")
except (ImportError, Exception):
    print("⚠️ Could not get API Key from Colab Secrets. Ensure it's set.")
    if not API_KEY:
        API_KEY = os.environ.get('GOOGLE_API_KEY')
        if API_KEY:
            print(f"[{datetime.now().time()}] ✅ API key configured from environment variable.")
        else:
            print("❌ ERROR: Could not configure API Key. The script cannot proceed.")

# --- Step 3: Model Configuration ---
print(f"[{datetime.now().time()}] ⚙️ Loading models... (This may take a moment)")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DINO_MODEL_ID = "IDEA-Research/grounding-dino-tiny"
dino_processor = AutoProcessor.from_pretrained(DINO_MODEL_ID)
dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(DINO_MODEL_ID).to(DEVICE)
print(f"[{datetime.now().time()}] ✅ Grounding DINO model loaded to {DEVICE}.")
MIDAS_MODEL_ID = "intel-isl/MiDaS"
midas_model = torch.hub.load(MIDAS_MODEL_ID, "DPT_Large", trust_repo=True).to(DEVICE)
midas_transform = torch.hub.load(MIDAS_MODEL_ID, "transforms", trust_repo=True).dpt_transform
print(f"[{datetime.now().time()}] ✅ MiDaS depth model loaded to {DEVICE}.")
MODELS = {"dino_processor": dino_processor, "dino_model": dino_model, "midas_model": midas_model, "midas_transform": midas_transform}
print("------------------------------------------")


# --- Step 4: Core Workflow Modules & Helper Functions ---

def display_side_by_side(original_path, annotated_path):
    if not os.path.exists(original_path) or not os.path.exists(annotated_path): return
    try:
        img1 = PILImage.open(original_path); img2 = PILImage.open(annotated_path)
    except Exception as e: print(f"Error opening images: {e}"); return
    size=(400, 400); img1.thumbnail(size); img2.thumbnail(size)
    dst = PILImage.new('RGB', (img1.width + img2.width, max(img1.height, img2.height)))
    dst.paste(img1, (0, 0)); dst.paste(img2, (img1.width, 0)); display(dst)

def display_image(image_path, title=""):
    if not os.path.exists(image_path) or image_path == "N/A": return
    try:
        img = PILImage.open(image_path); print(title); display(img)
    except Exception as e: print(f"Error opening image {image_path}: {e}")

def get_street_view_metadata(lat, lon, api_key):
    if not api_key: return None, "API Key not configured"
    params = {"location": f"{lat},{lon}", "key": api_key, "source": "outdoor"}
    try:
        response = requests.get("https://maps.googleapis.com/maps/api/streetview/metadata", params=params, timeout=15)
        response.raise_for_status(); metadata = response.json()
        return (metadata, "OK") if metadata.get("status") == "OK" else (None, f"Metadata API Status: {metadata.get('status', 'Unknown')}")
    except requests.exceptions.RequestException as e: return None, f"Error fetching metadata: {e}"

def get_street_view_image(lat, lon, api_key, save_path):
    if not api_key: return None, "API Key not configured"
    params = {"size": "640x640", "location": f"{lat},{lon}", "heading": "0", "pitch": "0", "source": "outdoor", "key": api_key}
    try:
        response = requests.get("https://maps.googleapis.com/maps/api/streetview", params=params, timeout=15)
        response.raise_for_status()
        if "staticmap" in response.url: return None, "Static map returned instead of Street View"
        image_path = os.path.join(save_path, f"image_{lat}_{lon}.png"); os.makedirs(save_path, exist_ok=True)
        with open(image_path, 'wb') as f: f.write(response.content)
        return image_path, "Success"
    except requests.exceptions.RequestException as e: return None, f"Error downloading image: {e}"

def calculate_iou(box_a, box_b):
    inter_x1 = torch.max(box_a[0], box_b[0]); inter_y1 = torch.max(box_a[1], box_b[1])
    inter_x2 = torch.min(box_a[2], box_b[2]); inter_y2 = torch.min(box_a[3], box_b[3])
    intersection_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)
    box_a_area = (box_a[2] - box_a[0]) * (box_a[3] - box_a[1]); box_b_area = (box_b[2] - box_b[0]) * (box_b[3] - box_b[1])
    union_area = box_a_area + box_b_area - intersection_area
    return intersection_area / (union_area + 1e-6)

def non_max_suppression(boxes, scores, labels, iou_threshold=0.5):
    if len(boxes) == 0: return torch.empty((0, 4)), torch.empty(0), []
    sorted_indices = torch.argsort(scores, descending=True); keep_indices = []
    while len(sorted_indices) > 0:
        current_idx = sorted_indices[0]; keep_indices.append(current_idx)
        if len(sorted_indices) == 1: break
        remaining_indices = sorted_indices[1:]
        ious = torch.tensor([calculate_iou(boxes[current_idx], boxes[i]) for i in remaining_indices], device=boxes.device)
        indices_to_keep = torch.where(ious <= iou_threshold)[0]
        sorted_indices = remaining_indices[indices_to_keep]
    if not keep_indices: return torch.empty((0, 4)), torch.empty(0), []
    final_keep_indices = torch.stack(keep_indices)
    final_labels = [labels[i] for i in final_keep_indices]
    return boxes[final_keep_indices], scores[final_keep_indices], final_labels

def detect_poles_in_image(image_path, models, device, confidence_threshold=0.35, iou_threshold=0.5):
    raw_image = PILImage.open(image_path).convert("RGB")
    text_prompt = "utility pole . telephone pole . light pole . wooden pole . concrete pole . pole with wires"
    inputs = models["dino_processor"](images=raw_image, text=text_prompt, return_tensors="pt").to(device)
    with torch.no_grad(): outputs = models["dino_model"](**inputs)
    results = models["dino_processor"].post_process_grounded_object_detection(outputs, inputs.input_ids, box_threshold=confidence_threshold, text_threshold=confidence_threshold, target_sizes=[raw_image.size[::-1]])
    boxes, labels, scores = results[0]["boxes"], results[0]["text_labels"], results[0]["scores"]
    if len(boxes) > 1:
        print(f"🔎 Found {len(boxes)} initial boxes. Applying NMS...")
        final_boxes, final_scores, final_labels = non_max_suppression(boxes, scores, labels, iou_threshold=iou_threshold)
        print(f"✅ Kept {len(final_boxes)} boxes after NMS.")
        return final_boxes, final_scores, final_labels, raw_image
    return boxes, scores, list(labels), raw_image

def estimate_distance_between_poles(image, boxes, models, device, fov=90):
    try:
        image_np = np.array(image)
        with torch.no_grad():
            transformed_image = models["midas_transform"](image_np).to(device)
            prediction = models["midas_model"](transformed_image)
            prediction = torch.nn.functional.interpolate(prediction.unsqueeze(1), size=image.size[::-1], mode="bicubic", align_corners=False).squeeze()
        depth_map = prediction.cpu().numpy()
        points_3d = []; image_width, image_height = image.size
        focal_length = (image_width / 2.0) / math.tan(math.radians(fov / 2))
        for box in boxes:
            cx = (box[0] + box[2]) / 2; cy = (box[1] + box[3]) / 2
            Z = (1 / (depth_map[int(cy), int(cx)] + 1e-6)) * 20
            X = (cx - image_width / 2.0) * Z / focal_length
            Y = (cy - image_height / 2.0) * Z / focal_length
            points_3d.append(np.array([X.item(), Y.item(), Z.item() if hasattr(Z, 'item') else Z]))
        if len(points_3d) == 2:
            distance_meters = np.linalg.norm(points_3d[0] - points_3d[1])
            return distance_meters * 3.28084
        return None
    except Exception as e:
        print(f"Error in estimate_distance_between_poles: {e}"); traceback.print_exc()
        return None

def draw_bounding_boxes(image, boxes, labels):
    draw = ImageDraw.Draw(image)
    for box, label in zip(boxes, labels):
        box = [int(round(coord.item())) for coord in box]; draw.rectangle(box, outline="#FF3333", width=3)
        text_bbox = draw.textbbox((box[0], box[1]), label, font_size=15); draw.rectangle(text_bbox, fill="#FF3333")
        draw.text((box[0], box[1]), label, fill="white", font_size=15)
    return image

def analyze_single_location(lat, lon, first_xmit_date_str, models, api_key, image_dir="images"):
    print(f"\n--- Analyzing: ({lat}, {lon}) | FirstXmit: {first_xmit_date_str} ---")
    # --- FIX: Ensure all keys are initialized to prevent KeyErrors on early returns ---
    result = {
        "Latitude": lat, "Longitude": lon, "FirstXmitDate": first_xmit_date_str,
        "ImageCaptureDate": "N/A", "NumberOfPoles": 0, "PoleTypes": "N/A",
        "EstimatedDistance(ft)": "N/A", "Status": "Undetermined", "Notes": "",
        "OriginalImagePath": "N/A", "AnnotatedImagePath": "N/A"
    }

    metadata, note = get_street_view_metadata(lat, lon, api_key)
    if not metadata:
        result.update({"Status": "Undetermined (No Metadata)", "Notes": note}); print(f"➡️ Result: {result['Status']} - {note}"); return result

    result["ImageCaptureDate"] = capture_date_str = metadata.get('date')
    try:
        if not capture_date_str or not first_xmit_date_str:
             raise ValueError("Date string is empty or None")
        first_xmit_date = pd.to_datetime(first_xmit_date_str).tz_localize(None)
        capture_date = datetime.strptime(capture_date_str, '%Y-%m')
        if first_xmit_date > capture_date:
            note = "Image Pre-dates Installation"; result.update({"Status": f"Undetermined ({note})", "Notes": note}); print(f"➡️ Result: {result['Status']}"); return result
    except (ValueError, TypeError) as e:
        note = f"Date parsing error: {e}"; result.update({"Status": f"Undetermined ({note})", "Notes": note}); print(f"➡️ Result: {result['Status']}"); return result

    loc_img_dir = os.path.join(image_dir, f"{lat}_{lon}")
    image_path, note = get_street_view_image(lat, lon, api_key, loc_img_dir)
    result["OriginalImagePath"] = image_path
    if not image_path: result.update({"Status": "Undetermined (No Image)", "Notes": note}); print(f"➡️ Result: {result['Status']} - {note}"); return result

    boxes, scores, labels, raw_image = detect_poles_in_image(image_path, models, DEVICE)
    result["NumberOfPoles"] = num_poles = len(boxes)
    print(f"🔎 Poles Detected: {num_poles}")
    if num_poles > 0:
        result["PoleTypes"] = ", ".join(labels)

    if num_poles == 0: result.update({"Status": "Undetermined (No Poles Found)", "Notes": "Detector found no poles."})
    elif num_poles == 1: result["Status"] = "Transfer Complete"
    elif num_poles == 2:
        distance_ft = estimate_distance_between_poles(raw_image, boxes, models, DEVICE)
        if distance_ft is not None:
            result["EstimatedDistance(ft)"] = round(float(distance_ft), 2); print(f"📏 Estimated distance: {result['EstimatedDistance(ft)']} ft")
            result["Status"] = "Transfer Pending" if distance_ft <= 6.0 else "Undetermined (Poles Not Adjacent)"
        else: result.update({"Status": "Undetermined", "Notes": "Distance calculation failed."})
    else: result["Status"] = "Undetermined (Multiple Poles Found)"

    if num_poles > 0:
        annotated_labels = [f"{label} ({score:.2f})" for label, score in zip(labels, scores)]
        annotated_image = draw_bounding_boxes(raw_image.copy(), boxes, annotated_labels)
        annotated_path = os.path.join(loc_img_dir, f"annotated_{lat}_{lon}_{result['Status']}.png".replace(" ", "_")); annotated_image.save(annotated_path)
        result["AnnotatedImagePath"] = annotated_path

    print(f"➡️ Result: {result['Status']}")
    return result

# --- Step 5: Main Execution Block ---
if API_KEY:
    try:
        csv_file_path = '/content/sample_data/SPANS_Review_20250707-50.csv'
        full_df = pd.read_csv(csv_file_path)
        num_samples = 50
        if len(full_df) < num_samples:
            print(f"⚠️ Warning: Input file has fewer than {num_samples} rows. Using all {len(full_df)} rows.")
            locations_df = full_df.copy()
        else:
            locations_df = full_df.sample(n=num_samples, random_state=42)
            print(f"✅ Selected {num_samples} random rows for analysis.")
        results_list = []; processed_coords = set()
        for index, row in locations_df.iterrows():
            # Ensure the row has the required columns and they are not empty/NaN
            if 'New_Lattitude' in row and pd.notna(row['New_Lattitude']) and \
               'New_Longitude' in row and pd.notna(row['New_Longitude']) and \
               'First_Xmit' in row and pd.notna(row['First_Xmit']):

                lat, lon = str(row['New_Lattitude']).strip(), str(row['New_Longitude']).strip()
                coord_tuple = (lat, lon)
                if coord_tuple in processed_coords: print(f"\n--- Skipping duplicate location: ({lat}, {lon}) ---"); continue
                processed_coords.add(coord_tuple)

                analysis_result = analyze_single_location(lat, lon, str(row['First_Xmit']).strip(), MODELS, API_KEY)
                results_list.append(analysis_result)

                # This display logic is now safe because the keys are always present
                if analysis_result["OriginalImagePath"] != "N/A":
                    print("🖼️  Displaying image comparison:")
                    if analysis_result["AnnotatedImagePath"] != "N/A":
                        display_side_by_side(analysis_result["OriginalImagePath"], analysis_result["AnnotatedImagePath"])
                    else:
                        display_image(analysis_result["OriginalImagePath"], title=f"Original Image for ({analysis_result['Latitude']}, {analysis_result['Longitude']})")
            else:
                print(f"⚠️ Skipping row {index} due to missing or empty required columns.")
            time.sleep(1)

        results_df = pd.DataFrame(results_list)
        cols_to_drop = ['OriginalImagePath', 'AnnotatedImagePath']
        results_df = results_df.drop(columns=[col for col in cols_to_drop if col in results_df.columns], errors='ignore')
        output_csv_path = "pole_transfer_analysis_report.csv"; results_df.to_csv(output_csv_path, index=False)
        print(f"\n\n✅ Analysis complete. Report saved to {output_csv_path}"); print("------------------------------------------"); display(results_df)
    except FileNotFoundError:
        print(f"\n❌ ERROR: Input file '{csv_file_path}' not found. Please ensure the file exists at this path.")
    except Exception as e:
        print(f"\n❌ An unexpected error occurred during execution: {e}")
        traceback.print_exc()
else:
    print("\nSkipping analysis because the API_KEY is not configured.")
